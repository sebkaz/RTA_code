{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app1.py\n"
     ]
    }
   ],
   "source": [
    "%%file app.py\n",
    "\n",
    "from flask import Flask\n",
    "\n",
    "# Create a flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Create an API end point\n",
    "@app.route('/', methods=['GET'])\n",
    "def hello_world():\n",
    "    return \"<h1> Hello World </h1>\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tekst przed opakowaniem\n",
      "tekst do opakowania\n",
      "tekst po opakowaniu\n"
     ]
    }
   ],
   "source": [
    "# DEKORATORY\n",
    "def dekorator(funkcja_do_opakowania):\n",
    "    def opakowanie():\n",
    "        print('tekst przed opakowaniem')\n",
    "        funkcja_do_opakowania()\n",
    "        print('tekst po opakowaniu')\n",
    "    return opakowanie\n",
    "\n",
    "def test():\n",
    "    print('tekst do opakowania')\n",
    "\n",
    "\n",
    "f = dekorator(test)\n",
    "f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tekst przed opakowaniem\n",
      "inny tekst do opakowania\n",
      "tekst po opakowaniu\n"
     ]
    }
   ],
   "source": [
    "@dekorator\n",
    "def test2():\n",
    "    print('inny tekst do opakowania')\n",
    "\n",
    "test2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subprocess.Popen([\"python\", \"app1.py\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://127.0.0.1:5000/hello\")\n",
    "response.content\n",
    "\n",
    "p.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app2.py\n"
     ]
    }
   ],
   "source": [
    "%%file app2.py\n",
    "\n",
    "from flask import Flask\n",
    "from flask import request\n",
    "\n",
    "# Create a flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Create an API end point\n",
    "@app.route('/hello', methods=['GET'])\n",
    "def say_hello():\n",
    "    name = request.args.get(\"name\", \"\")\n",
    "    title = request.args.get(\"title\", \"\")\n",
    "    if name:\n",
    "        resp = f\"Hello {title} {name}\" if title else f\"Hello {name}\"\n",
    "    else:\n",
    "        resp = f\"Hello {title}\" if title else \"Hello\"\n",
    "    return resp\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subprocess.Popen([\"python\", \"app2.py\"])\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:5000/hello?name=Sebastian\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "res = r.get(http://localhost:5000/api/predict?&sl=4.5&pl=1.3)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl 'localhost:5000/api/predict?&sl=4.5&pl=1.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "Downloading: 100%|██████████| 629/629 [00:00<00:00, 69.6kB/s]\n",
      "Downloading: 100%|██████████| 255M/255M [00:18<00:00, 14.7MB/s] \n",
      "Downloading: 100%|██████████| 48.0/48.0 [00:00<00:00, 17.7kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 259kB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.999871015548706}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "senti_classifier = pipeline('sentiment-analysis')\n",
    "senti_classifier(\"AWESOME!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.999871015548706},\n",
       " {'label': 'NEGATIVE', 'score': 0.7939676642417908},\n",
       " {'label': 'NEGATIVE', 'score': 0.9784122109413147}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_docs = [\n",
    "    \"AWESOME!!!\", \n",
    "    \":(\", \n",
    "    \"I like this product but I won't buy it again\"]\n",
    "senti_classifier(senti_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 953/953 [00:00<00:00, 307kB/s]\n",
      "Downloading: 100%|██████████| 638M/638M [00:46<00:00, 14.3MB/s] \n",
      "Downloading: 100%|██████████| 39.0/39.0 [00:00<00:00, 7.09kB/s]\n",
      "Downloading: 100%|██████████| 851k/851k [00:00<00:00, 1.63MB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 8.60kB/s]\n"
     ]
    }
   ],
   "source": [
    "senti_classifier = pipeline(\"sentiment-analysis\",\n",
    "model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.8773106932640076},\n",
       " {'label': '3 stars', 'score': 0.24814485013484955},\n",
       " {'label': '3 stars', 'score': 0.5084365010261536},\n",
       " {'label': '3 stars', 'score': 0.3329271674156189},\n",
       " {'label': '1 star', 'score': 0.41605350375175476},\n",
       " {'label': '5 stars', 'score': 0.6315645575523376}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_docs = [\n",
    "    \"AWESOME!!!\", \n",
    "    \":(\", \n",
    "    \"I like this product but I won't buy it again\",\n",
    "    \"to chyba żart\",\n",
    "    \"jesteś niepoważny\",\n",
    "    \"ten sprzęt jest tak świetny, że nie da się go nawet włączyć\"\n",
    "]\n",
    "senti_classifier(senti_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.76k/1.76k [00:00<00:00, 228kB/s]\n",
      "Downloading: 100%|██████████| 1.14G/1.14G [01:25<00:00, 14.2MB/s]\n",
      "Downloading: 100%|██████████| 26.0/26.0 [00:00<00:00, 7.95kB/s]\n",
      "Downloading: 100%|██████████| 878k/878k [00:00<00:00, 913kB/s]  \n",
      "Downloading: 100%|██████████| 446k/446k [00:01<00:00, 374kB/s]  \n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\",\n",
    "model=\"sshleifer/distilbart-cnn-12-6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_doc = (\"\"\"\n",
    "A regular expression (shortened as regex or regexp; also \n",
    "referred to as rational expression) is a sequence of characters \n",
    "that specifies a search pattern. Usually such patterns are used \n",
    "by string-searching algorithms for 'find' or 'find and replace' \n",
    "operations on strings, or for input validation. It is a technique \n",
    "developed in theoretical computer science and formal language \n",
    "theory. The concept arose in the 1950s when the American \n",
    "mathematician Stephen Cole Kleene formalized the description \n",
    "of a regular language. The concept came into common use with Unix \n",
    "text-processing utilities. Different syntaxes for writing regular \n",
    "expressions have existed since the 1980s, one being the POSIX \n",
    "standard and another, widely used, being the Perl syntax.\n",
    "Regular expressions are used in search engines, search and replace \n",
    "dialogs of word processors and text editors, in text processing \n",
    "utilities such as sed and AWK and in lexical analysis.\n",
    "Many programming languages provide regex capabilities either \n",
    "built-in or via libraries, as it has uses in many situations.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' A regular expression is a sequence of characters that specifies a search pattern . It is a technique developed in theoretical computer science and formal language . The concept arose in the 1950s when Stephen Cole Kleene formalized the description of a regular language'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(sum_doc, min_length=10, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' A regular expression is a sequence of characters that specifies a search pattern . It is a technique developed in theoretical computer science and formal language . The concept arose in the 1950s when Stephen Cole Kleene formalized the description of a regular language . Different syntaxes for writing regular expressions have existed since the 1980s .'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(sum_doc, min_length=10, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.11k/1.11k [00:00<00:00, 599kB/s]\n",
      "Downloading: 100%|██████████| 295M/295M [00:15<00:00, 19.6MB/s] \n",
      "Downloading: 100%|██████████| 42.0/42.0 [00:00<00:00, 22.6kB/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/air/Documents/GitHub/RTA_code/rozdzial 4/rozdzial4.ipynb Cell 19'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/air/Documents/GitHub/RTA_code/rozdzial%204/rozdzial4.ipynb#ch0000021?line=0'>1</a>\u001b[0m translator \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mtranslation_pl_to_en\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/air/Documents/GitHub/RTA_code/rozdzial%204/rozdzial4.ipynb#ch0000021?line=1'>2</a>\u001b[0m model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHelsinki-NLP/opus-mt-pl-en\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/air/Documents/GitHub/RTA_code/rozdzial%204/rozdzial4.ipynb#ch0000021?line=3'>4</a>\u001b[0m translator(\u001b[39m\"\u001b[39m\u001b[39mcoś bym napisał, ale mi się nie chce\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/pipelines/__init__.py:598\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/pipelines/__init__.py?line=594'>595</a>\u001b[0m             tokenizer_identifier \u001b[39m=\u001b[39m tokenizer\n\u001b[1;32m    <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/pipelines/__init__.py?line=595'>596</a>\u001b[0m             tokenizer_kwargs \u001b[39m=\u001b[39m model_kwargs\n\u001b[0;32m--> <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/pipelines/__init__.py?line=597'>598</a>\u001b[0m         tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/pipelines/__init__.py?line=598'>599</a>\u001b[0m             tokenizer_identifier, revision\u001b[39m=\u001b[39;49mrevision, use_fast\u001b[39m=\u001b[39;49muse_fast, _from_pipeline\u001b[39m=\u001b[39;49mtask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokenizer_kwargs\n\u001b[1;32m    <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/pipelines/__init__.py?line=599'>600</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/pipelines/__init__.py?line=601'>602</a>\u001b[0m \u001b[39mif\u001b[39;00m load_feature_extractor:\n\u001b[1;32m    <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/pipelines/__init__.py?line=602'>603</a>\u001b[0m     \u001b[39m# Try to infer feature extractor from model or config name (if provided as str)\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/pipelines/__init__.py?line=603'>604</a>\u001b[0m     \u001b[39mif\u001b[39;00m feature_extractor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:551\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py?line=548'>549</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m tokenizer_class_py\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py?line=549'>550</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py?line=550'>551</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py?line=551'>552</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py?line=552'>553</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39min order to use this tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py?line=553'>554</a>\u001b[0m             )\n\u001b[1;32m    <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py?line=555'>556</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py?line=556'>557</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m to build an AutoTokenizer.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py?line=557'>558</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m TOKENIZER_MAPPING\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/air/Documents/GitHub/RTA_code/venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py?line=558'>559</a>\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation_pl_to_en\",\n",
    "model=\"Helsinki-NLP/opus-mt-pl-en\")\n",
    "\n",
    "translator(\"coś bym napisał, ale mi się nie chce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator2 = pipeline(\"translation_en_to_pl\",\n",
    "model=\"Helsinki-NLP/opus-mt-en-zlw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator2(\"I'd write something but I don't want to.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_doc = \"Knock knock. Who's there?\"\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(gen_doc, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e1143f982d67ed63b394819c0055b556226ea2b15cfc376a81c44e9359cfc8e9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
